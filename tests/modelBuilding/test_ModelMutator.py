import matplotlib.pyplot as plt
from torch import nn
import torchvision.models as models
from pyTorchAutoForge.model_building import TemplateDeepNet

import matplotlib
import pytest
from torch import nn
from pyTorchAutoForge.model_building.ModelMutator import ModelMutator, EnumMutations
matplotlib.use('agg')  # or 'Qt5Agg'
plt.ion()

########### Autogenerated test harnesses ###########       
# A simple model with a single BatchNorm2d layer
class SimpleBNModel(nn.Module):
    def __init__(self):
        super(SimpleBNModel, self).__init__()
        self.conv = nn.Conv2d(3, 8, 3, 1, 1)
        self.bn = nn.BatchNorm2d(8)

    def forward(self, x):
        return self.bn(self.conv(x))

# A model with nested BatchNorm layers
class NestedBNModel(nn.Module):
    def __init__(self):
        super(NestedBNModel, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(3, 16, 3, 1, 1),
            nn.BatchNorm2d(16),
            nn.ReLU()
        )
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, 3, 1, 1),
            nn.BatchNorm2d(32),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        return x

def count_bn_layers(module: nn.Module) -> int:
    return sum(1 for m in module.modules() if isinstance(m, nn.BatchNorm2d))

def count_gn_layers(module: nn.Module) -> int:
    return sum(1 for m in module.modules() if isinstance(m, nn.GroupNorm))

def test_simple_model_mutation():
    # Create a simple model with one BatchNorm2d layer
    model = SimpleBNModel()
    # Ensure BN exists before mutation
    assert count_bn_layers(model) == 1

    # Perform mutation: Replace BN with GN. Use numOfGroups that divides channels (8)
    mutator = ModelMutator(model, numOfGroups=8, mutation_type=EnumMutations.BNtoGN)
    mutated_model = mutator.mutate()

    # After mutation, ensure no BatchNorm2d remains and at least one GroupNorm exists.
    assert count_bn_layers(mutated_model) == 0
    assert count_gn_layers(mutated_model) > 0

def test_nested_model_mutation():
    # Create a nested model with two BatchNorm2d layers
    model = NestedBNModel()
    # Ensure BN exists before mutation
    initial_bn_count = count_bn_layers(model)
    assert initial_bn_count == 2

    # Perform mutation: Replace BN with GN. Choose numOfGroups that divides 16 and 32.
    mutator = ModelMutator(model, numOfGroups=8,
                           mutation_type=EnumMutations.BNtoGN)
    mutated_model = mutator.mutate()

    # Ensure that all BatchNorm layers are replaced by GroupNorm layers.
    assert count_bn_layers(mutated_model) == 0
    assert count_gn_layers(mutated_model) == 2

def test_invalid_group_division():
    # Create a model with a BatchNorm2d layer with channels that don't fit the default groups.
    class UnusualBNModel(nn.Module):
        def __init__(self):
            super(UnusualBNModel, self).__init__()
            # 10 channels: not divisible by 32, so the mutator should choose an alternative group.
            self.conv = nn.Conv2d(3, 10, 3, 1, 1)
            self.bn = nn.BatchNorm2d(10)

        def forward(self, x):
            return self.bn(self.conv(x))

    model = UnusualBNModel()
    mutator = ModelMutator(model, numOfGroups=32)
    mutated_model = mutator.mutate()

    # After mutation, ensure BN is replaced. Even though 32 doesn't divide 10,
    # the mutator should have chosen a suitable number via find_divisible_groups.
    assert count_bn_layers(mutated_model) == 0
    # Verify that a GroupNorm layer exists and its num_groups divides the channel count.
    for m in mutated_model.modules():
        if isinstance(m, nn.GroupNorm):
            assert m.num_channels % m.num_groups == 0
##################################################################

def main():

    # Test workflow
    resAdapter = SimpleBNModel() 
    print("Adapter model: \n", resAdapter)

    # Get model backbone from torchvision
    feature_extractor = models.efficientnet_b0(weights=True)  # Load model without weights

    # Remove last Linear classifier
    feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])

    # TEST: Mutate all BatchNorm layers to GroupNorm
    feature_extractor = (ModelMutator(feature_extractor, 32, mutation_type=EnumMutations.BNtoGN)).mutate()

    print("EfficientNet B model modified to use GN: \n", feature_extractor)
    efficientNet_output_size = 1280

    # Define DNN regressor for centroid prediction

    headCentroid_config = {
        'input_size': efficientNet_output_size,
        'useBatchNorm': False,
        'alphaDropCoeff': 0.0,
        'alphaLeaky': 0.0,
        'outChannelsSizes': [256, 128, 32, 16, 2]
    }

    centroidRegressor = TemplateDeepNet(headCentroid_config)
    print("Centroid regressor model: \n", headCentroid_config)

    model = nn.Sequential(
        resAdapter,
        feature_extractor,
        centroidRegressor
    )

    print('\n MODEL AFTER MUTATION')
    print(model)

    # Run tests
    test_simple_model_mutation()
    test_nested_model_mutation()
    test_invalid_group_division()
    



if __name__ == '__main__':
    main()